{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Pre and Post Simulation Tasks on Energy Exemplar Cloud. \n",
    "Pre and Post Simulation tasks enables you to run custom logic before or after a simulation on cloud. Python scripts are supported by default, but you also have abilty to run native code compiled for Linux and x64 architecture. All scripts or artifacts needed for your tasks need to be uploaded into Energy Exemplar Datahub, which will be accessible during run on the cloud. Datahub provides a seamless storage mechansism and offers versioning and account control capability. Multiple tasks can be configured to run before or after a run. Task output can be manually uploaded to Datahub, or you can retrieve data via CLI manually. \n",
    "\n",
    "### Covered in this Example\n",
    "1. Python libraries, environment variables and SDK setup\n",
    "2. Setting SDK/CLI Environment\n",
    "3. Authentication (SSO)\n",
    "4. Authentication (Service Principal)\n",
    "5. Configure Datahub Sync directory\n",
    "6. Sync local content with Datahub\n",
    "7. List Datahub content\n",
    "8. Configure Simulation and Tasks\n",
    "9. Enqueue Simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the Python SDK client, Setup variables. <a class=\"anchor\" id=\"environment-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, utilities\n",
    "from pathlib import Path\n",
    "from eecloud.cloudsdk import CloudSDK, SDKBase\n",
    "from eecloud.models import *\n",
    "\n",
    "repo_name = \"JupyterSamples\"\n",
    "root_path = os.environ.get(\"HOME\")\n",
    "jupyter_user = os.environ.get(\"JUPYTERHUB_USER\")\n",
    "project_root = os.path.join(root_path, repo_name, \"PreAndPostTasks\")\n",
    "\n",
    "local_path = os.path.join(project_root, \"Scripts\") \n",
    "datahub_path = f\"{repo_name}/{jupyter_user}\"\n",
    "simulation_body = os.path.join(local_path, \"simulation_body.json\")\n",
    "\n",
    "solution_download_directory = os.path.join(project_root, \"Downloads\")\n",
    "cli_path = os.environ.get(\"cloud_cli_path\")\n",
    "environment: str = \"NA\"\n",
    "\n",
    "if environment is None:\n",
    "    raise Exception(\"environment value must be configured\")\n",
    "\n",
    "# Setting an environment variable named \"cloud_cli_path\" with full path to CLI prevented need to configure SDK manually\n",
    "pxc = CloudSDK(cli_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env_response: list[CommandResponse[Contracts_EnvironmentResponse]] = pxc.environment.set_user_environment(environment)\n",
    "    env_data: Contracts_EnvironmentResponse = pxc.environment.get_final_response(env_response)\n",
    "    print(f\"{env_data.EventData.Environment} selected, please authenticate\")\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login to PLEXOS Cloud using SSO - Use this option on Windows desktop. Not for Hub or automations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE for Local Notebook Use.\n",
    "#try:\n",
    "#    login_response: list[CommandResponse[Contracts_LoginResponse]] = pxc.auth.login()\n",
    "#    login_data: Contracts_LoginResponse = SDKBase.get_response_data(login_response)\n",
    "#    print(f\"Tenant: {login_data.TenantName}, User: {login_data.UserName}\")\n",
    "#except Exception as ex:\n",
    "#    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login to PLEXOS Cloud using Client credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee_tenant_id = '5b3f24e2-a55d-452b-9c4b-2cdd5d35040c'\n",
    "    ee_client_id = 'b75bb1d7-b0d9-43ac-b8c6-36007f147980'\n",
    "    ee_client_secret = '5oulqrzZSpBdUJNPS1N97AUDJaLR05d3BXdlLrmOUzY='\n",
    "    \n",
    "    command_responses = pxc.auth.login_client_credentials(tenant_id=ee_tenant_id, client_id=ee_client_id, client_secret=ee_client_secret, use_client_credentials=True, print_message=True)\n",
    "    last_command_response: CommandResponse[Contracts_LoginResponse] = pxc.auth.get_final_response(command_responses)\n",
    "    if last_command_response is not None and last_command_response.Status == \"Success\":\n",
    "        data: Contracts_LoginResponse = last_command_response.EventData\n",
    "        print(f\"{data.TenantName} logged into {data.Environment}\")\n",
    "        \n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Datahub to local folder sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Attempting to map local path: {local_path} to {datahub_path} on Datahub\")\n",
    "    map_response: list[CommandResponse[Contracts_DatahubMapResponse]] = pxc.datahub.map_folder(local_path, datahub_path, print_message=False)\n",
    "    map_data: Contracts_DatahubMapResponse = SDKBase.get_response_data(map_response)\n",
    "\n",
    "    if map_data is not None:\n",
    "        print(f\"Map success: {map_data.Success}, Local Path: {map_data.LocalPath}, Remote Path: {map_data.RemotePath}, Patterns: {map_data.Patterns}\")\n",
    "    else:\n",
    "        print(f\"Mapping already exists!\")\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sync data in between mapped local folder and Datahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sync_response: list[CommandResponse[Contracts_DatahubCommandResponse]] = pxc.datahub.sync(local_path_to_sync=local_path, print_message=True)\n",
    "    sync_data: Contracts_DatahubCommandResponse = SDKBase.get_response_data(sync_response)\n",
    "    print(f\"Sync Status: {sync_data.DatahubCommandStatus.value}\")\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Datahub files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    glob_pattern_1: str = f\"{datahub_path}/**\" # this is a wildcard search based on previously provided path\n",
    "\n",
    "    search_response: list[CommandResponse[Contracts_DatahubSearchResponse]] = pxc.datahub.search([f\"{datahub_path}/**\"], print_message=False)\n",
    "    search_data: Contracts_DatahubSearchResponse = pxc.datahub.get_final_response(search_response)\n",
    "    \n",
    "    if search_data.EventData.DatahubSearchResults is not None:\n",
    "        for item in search_data.EventData.DatahubSearchResults:\n",
    "            if item.IsDeleted == False:\n",
    "                print(f\"{item.RelativePath} - Total Versions: {len(item.Versions)} : Latest Version: {item.LatestServerVersion}\")\n",
    "    else:\n",
    "        raise Exception(\"Failure listing datahub files\") \n",
    "            \n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Tasks in Simulation Payload <a class=\"anchor\" id=\"define-tasks\"></a>\n",
    "\n",
    "Manually update the [simulation_body.json](Scripts/simulation_body.json) located in the Scripts directory. The following must be updated\n",
    "- StudyId\n",
    "- ChangesetId\n",
    "- Models\n",
    "- SimulationData\n",
    "- SimulationEngine\n",
    "- The tasks under SimulationTasks must be updated to include your Datahub path names. Documentation here: [Task Documentation](https://portal.energyexemplar.com/unified-help/plexos-cloud/#t=Simulations%2FPre-_and_Post-Simulation_Tasks.htm&rhsearch=pre%20and%20post&ux=search) \n",
    "  - TaskType: - Pre or Post\n",
    "  - Files: Datahub Path and Version\n",
    "  - Arguments: If python script needs to be executed, the arguments must contain \"python3 yourscript.py\"\n",
    "\n",
    "*** If new scripts are added, the sync process must be run again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "...\n",
    "{\n",
    "    \"Name\": \"TASK NAME\",\n",
    "    \"TaskType\": \"Pre\",\n",
    "    \"Files\": [\n",
    "        {\n",
    "            \"Path\": \"JupyterSamples/<user.name>/query_write_memberships.py\",\n",
    "            \"Version\": null\n",
    "        },\n",
    "        {\n",
    "            \"Path\": \"JupyterSamples/<user.name>/requirements.txt\",\n",
    "            \"Version\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"Arguments\": \"python3 query_write_memberships.py\",\n",
    "    \"ContinueOnError\": true,\n",
    "    \"ExecutionOrder\": 1\n",
    "}\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enqueue Simulation, Wait for Completion. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    enqueue_response : list[CommandResponse[Contracts_EnqueueSimulationResponse]] = pxc.simulation.enqueue_simulation(simulation_body, print_message=False)\n",
    "    enqueue_data: Contracts_EnqueueSimulationResponse = SDKBase.get_response_data(enqueue_response)\n",
    "    \n",
    "    simulation = enqueue_data.SimulationStarted[0]\n",
    "    simulation_id = simulation.Id.Value\n",
    "    execution_id = simulation.ExecutionId.Value\n",
    "\n",
    "    print(f\"Simulation: {simulation_id} enqueued\")\n",
    "\n",
    "    utilities.wait_simulation_finish(pxc, simulation_id)\n",
    "    simulation_result = utilities.get_simulation(pxc, simulation_id=simulation_id)\n",
    "\n",
    "    #optionally if multiple simulations were enqueued we can wait for all executions to finish \n",
    "    #execution_result = utilities.get_executions(pxc, execution_id=simulation_result.ExecutionId.Value)\n",
    "    #execution_result = utilities.wait_execution_finish(pxc, execution_id=simulation_result.ExecutionId.Value)\n",
    "\n",
    "    if simulation_result.Status in ['CompletedSuccess']:\n",
    "        solution_id = simulation_result.ModelIdentifiers[0].Id\n",
    "        print(f\"Simulation complete. {simulation_result.Status} Downloading artifacts for solution: {solution_id}\")  \n",
    "        sim_directory = f\"{solution_download_directory}/{simulation_id}\"\n",
    "        Path(sim_directory).mkdir(parents=True, exist_ok=True)\n",
    "        utilities.download_solution_data(pxc, solution_id , sim_directory)\n",
    "        print(f\"Solution data downloaded to {solution_download_directory}\")\n",
    "    else:\n",
    "        print(f\"Simulation finished: {simulation_result.Status} - Possible failure\")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus. Querying downloaded artifacts for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with duckdb.connect() as con:\n",
    "    #convert Parquet to csv\n",
    "    con.execute(f\"COPY (select * from '{solution_download_directory}/**/solution_data*.parquet') TO '{solution_download_directory}/solution_data.csv' (DELIMITER ',');\")\n",
    "    \n",
    "    #aggregate Generation by Date\n",
    "    query: str = f\"\"\"\n",
    "    SELECT date_trunc('month', StartDate) as Month, sum(TotalValue) as TotalGeneration \n",
    "    FROM '{solution_download_directory}/**/solution_data*.parquet' \n",
    "    where BandId = 1 \n",
    "    group by date_trunc('month', StartDate);\"\"\"\n",
    "    \n",
    "    #show tabular results\n",
    "    con.sql(query).show() \n",
    "\n",
    "    result = con.sql(query).to_df()\n",
    "    #plot generation by month   \n",
    "    plt.figure(figsize=(10, 6), dpi=100)\n",
    "    plt.plot(result)\n",
    "    plt.title(\"Generation by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"TotalGeneration\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
